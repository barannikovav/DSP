
\section{Марковская цепь как случайный процесс}

\textbf{Автор:} Лирисман Карина Сергеевна, Б-01-008

\subsection{Введение}

	Идея А.А. Маркова, лежащая в основе всей развиваемой теории марковских процессов, состоит в том, что выделяется класс процессов, для которых эволюция во времени может быть описана следующим образом: поведение процесса после момента $t$ определяется не всей его предысторией, а лишь значением,которое процесс принял в момент времени $t$. Здесь уместна аналогия с классическим описанием движения “частицы”, поведение которой после момента t опре­деляется лишь ее положением (координатами) и скоростью в момент $t$. Если время дискретно $(t = 0, 1,...)$ , то сказанное особенно наглядно: изменение состояний изучаемого объекта представляется последовательностью шагов, где каждый шаг
определяется предыдущим. Отсюда и возникло понятие \textit{цепи Маркова}, послужив­шее основой разнообразных дальнейших обобщений.


\subsection{Случайный процесс}
Пусть $\xi = \xi (t)$ - случайная величина, заданная на вероятностном пространстве $(\Omega , \mathscr{F}, P)$ и $F_{\xi} (x) = P \lbrace \omega : \xi (\omega) \leq x \rbrace$ - ее функция распределения \textit{(А.Н. Ширяев 'Вероятность'  1957, стр. 260)}. Существует случайная величина, имеющая функцию $F(x)$ своей функцией распределения, то есть существует вероятностное пространство $(\Omega , \mathscr{F}, P)$ и случайная величина $\xi = \xi (\omega)$ на нем такие, что 
\begin{center}
$P \lbrace \omega : \xi (\omega) \leq x \rbrace = F(x)$.
\end{center}

Случайный процесс с временным интервалом $T$ - совокупность случайных величин $X = (\xi_{t})_{t \in T }$, где $T$ - некоторое подмножество числовой прямой (А.Н. Ширяев 'Вероятность' 1957, стр 194 определение 3).

Итак, $X = (\xi_{t})_{t \in T }$ - случайный процесс в смысле данного выше определения, заданный на вероятностном пространстве $(\Omega , \mathscr{F}, P)$ для $t \in T \subseteq R$.

С физической точки зрения наиболее важной вероятностной характеристикой случайного процесса является набор 

$\lbrace F_{t_{1}, ..., t_{n}}(x_1, ..., x_n) \rbrace$ его конечных функций распределения
\begin{center}
 $ F_{t_{1}, ..., t_{n}}(x_1, ..., x_n) = P \lbrace \omega : \xi_{t_1} \leq x_1, ..., \xi_{t_n} \leq x_n \rbrace$,
\end{center}
заданных для всех наборов $t_1, ... t_n$ c $t_1 \le t_2 \le ... \le t_n$.

Всякую функцию $F = F_n (x_1, ..., x_n)$, удовлетворяющую условиям ( \textit{А.Н. Ширяев 'Вероятность' 1957 стр 175}):
\begin{enumerate}
\item $	\Delta_{a_i b_i}...\Delta_{a_n b_n} F_n (x_1... x_n) \geq 0$, где $	\Delta_{a_i b_i}$ : $R^n \rightarrow R$ - разностный оператор, действующий по формуле $(a_i \leq b_i)$:
\begin{center}
$\Delta_{a_i b_i}...\Delta_{a_n b_n} F_n (x_1... x_n) = F_n(x_1, ..., x_{i-1}, b_i, x_{i+1} ...) - F_n(x_1, ..., x_{i-1}, a_i, x_{i+1} ...) $
\end{center}
\item $F_n(x^{(k)}) \downarrow F_n(x), k  \rightarrow \infty$
\item $F_n(+\infty , ..., +\infty) = 1$
\item $\lim\limits_{x \downarrow y} F_n(x-1, ..., x_n) = 0$,
\end{enumerate}
будем называть $n$ - мерной функцией распределения в $R^n$.

Из формулы выше видно, что для каждого набора $t_1, ... t_n$ c $t_1 \le t_2 \le ... \le t_n$ функции $F_{t_{1}, ..., t_{n}}(x_1, ..., x_n)$ являются $n$-мерными функциями распределения в смысле данного выше определения, и что набор 
\begin{center}
 $ F_{t_{1}, ..., t_{n}}(x_1, ..., x_n) = F_{{t_1}, ... {t_n}} (x_1 , ..., x_k, +\infty , ..., +\infty)$, 
\end{center}
где $k < n$.

Естественно теперь поставить вопрос о том, при каких условиях заданное семейство функций распределения $F_{t_{1}, ..., t_{n}}(x_1, ..., x_n)$ в смысле упомянутого определения может быть семейством конечномерных функций распределения некоторого случайного процесса? Для ответа на вопрос приведем формулировку теоремы Колмогорова о существовании процесса. 

\subsection{Теорема Колмогорова}

\begin{theorem}[Теорема Колмогорова о существовании процесса] Пусть 
\begin{center}
$\lbrace F_{t_{1}, ..., t_{n}}(x_1, ..., x_n) \rbrace $, где $t_i \in T \subseteq R, t_1 \le t_2 \le ... \le t_n, n \geq 1$,
\end{center}
заданное семейство конечномерных функций распределения, удовлетворяющих условиям согласованности. Тогда существует вероятностное пространство $(\Omega , \mathscr{F}, P)$ и случайный процесс $X = (\xi_{t})_{t \in T }$, такие что 
\begin{center}
$P \lbrace \omega : \xi_{t_1} \leq x_1, ..., \xi_{t_n} \leq x_n \rbrace = F_{{t_1}, ..., {t_n}} (x_1, ..., x_n) $
\end{center}
\end{theorem}

Сформулируем также два следствия данной теоремы.

\begin{corollary} Пусть $F_1 (x),  F_2 (x), ...$ - последовательность одномерных функций распределения. Тогда существуют вероятностное пространство $(\Omega , \mathscr{F}, P)$ и последовательность независимых случайных величин $\xi_1,  \xi_2, ... $ такие, что
	\begin{center}
	$P \lbrace \omega : \xi_i (w) \leq x \rbrace = F_i (x)$
	\end{center}
	В частности, существует вероятностное пространство $(\Omega , \mathscr{F}, P)$, на котором определена бесконечная последовательность бернуллиевских случайных величин. Отмечу, что в качестве $\Omega$ можно здесь взять пространство
	\begin{center}
	$\Omega = \lbrace \omega : \omega = (a_1, a_2, ...), a_i = 0, 1 \rbrace$
	\end{center}
\end{corollary}

Для доказательства данного следствия достаточно положить $F_{1, ..., n} (x_1, ..., x_n) = F_1 (x_1) ... F_n (x_n)$ и применить теорему Колмогорова о существовании процесса.

\begin{corollary} Пусть $T = [0, \infty)$ и $\lbrace p(s, x; t, B)\rbrace$ - семейство неотрицательных функций, определенных для $s, t \in T, t > s, x \in R, B \in \mathscr{B}(R)$ и удовлетворяющих следующим условиям:
	\begin{enumerate}
	\item $p(s, x; t, B)$ является при фиксированных $s, x$ и $t$ вероятностной мерой по $B$
	\item при фиксированных $s, t$ и $B$  $p(s, x; t, B)$ является борелевской функцией по $x$
	\item для всех $0 \leq s \le t \le \tau$ и $B \in \mathscr{B}(R)$ выполняется уравнение Колмогорова - Чэпмена
	\begin{center}
	$p(s, x; t, B) = \int\limits_{R}^{ } p(s, x; t, dy)p(t, y; \tau, B)$
	\end{center}
	\end{enumerate}
	
	И пусть $\pi = \pi (B)$ - вероятностная мера на $R$, $\mathscr{B}(R)$. Тогда существуют вероятностное пространство $(\Omega , \mathscr{F}, P)$ и случайный процесс $X = \lbrace \xi_t \rbrace_{t \geq 0}$ на нем такие, что для $0 = t_0 \le t_1 \le ... \le t_n$, 
	\begin{center}
	$P \lbrace \xi_{t_0} \leq x_0, \xi_{t_1} \leq x_1, ..., \xi_{t_n} \leq x_n \rbrace = \int\limits_{-\infty}^{x_0} \pi (dy_{0}) \int\limits_{-\infty}^{x_1} p(0, y_0; t_1, dy_1) ... \int\limits_{-\infty}^{x_n} p(t_{n-1}, y_{n_1}; t_n, dy_n)$
	\end{center}
\end{corollary}

Так постоенный процесс $X$ называется \textit{марковским процессом} \textit{(А.Н. Ширяев 'Вероятность' 1957 стр. 263)} 

\subsection{Цепь Маркова как случайный процесс}
В зависимости от того, непрерывное или дискретное множество значений принимает случайный процесс $X = (\xi_{t})_{t \in T }$ и его параметр время $t$, различают виды Марковских случайных процессов. Цепь Маркова - дискретный процесс с дискретным временем. В данном случае переходы системы из одного в другое состояние возможно в строго определенные моменты времени $t_0$, $t_1$, …, $t_n$, а случайный процесс $X$ в промежутках между указанными моментами времени сохраняет свое состояние.

Если каждое состояние системы в k-й момент времени свяжем с вероятностью, то получим вероятности состояний $P(k)_1$, $P(k)_2$, $P(k)_3$, … , $P(k)_n$. Для любого момента времени 
\begin{center}
$P(k)_1$ + $P(k)_2$ + … + $P(k)_n$ = 1
\end{center}

Существуют функции $P_{n+1} (x; B)$ - регулярные условные вероятности \textit{(А.Н. Ширяев Вероятность 1957 стр 530)}, являющиеся при фиксированном $x$ мерами на $(R,\mathscr{B}(R))$  и при фиксированном $B$ измеримыми функциями по $x$, такие, что
\begin{center}
$P(X_{n+1} \in B | X_n) = P_{n+1}(X_n; B)$ (Р-п. н.)
\end{center} 

Функции $P_n = P_n (x; B), n \geq 0$, называют переходными функциями \textit{(А.Н. Ширяев Вероятность 1957 стр 530)} и в том случае, когда они совпадают,  т.е. $P_1 = P_2 = ... $, соответствующую марковскую цепь $X$ принято называть однородной по времени.

Вероятностная картина возможных состояний системы и ее переходов может быть задана матрицей Р, элементами которой являются переходные
вероятности: 

\begin{center}

$ P(k)= \begin{bmatrix}
		\ p(k)_{11}& \ p(k)_{12}...& \ p(k)_{1n}\\
		\ p(k)_{21}& \ p(k)_{22}...& \ p(k)_{2n}\\
		\ & \ ... & \ \\
		\ p(k)_{n1}& \ p(k)_{n2}...& \ p(k)_{nn}\\
		\end{bmatrix}
$
\end{center}

Матрица переходных вероятностей обладает следующими свойствами:
\begin{enumerate}
\item cумма вероятностей, стоящих в каждой строке матрицы равна единице
\item на главной диагонали матрицы стоят вероятности того, что система не выйдет из состояния,  а останется в нем
\item если переходная вероятность $p_{ij}(k)$ = 0, то это означает, что на данном шаге система не может перейти из одного состояния в другое.
\end{enumerate}

Имея матрицу переходных вероятностей, данные о начальном состоянии системы, можно найти все возможные вероятности состояний системы для любого момента времени. Для этого, в случае однородного Марковского процесса используют следующее уравнение:
\begin{center}
$p(k)_j = \sum_{i=1}^n p_i (k-1)  p_{ij} , j = 1, 2, ... , n$
\end{center}

Итак, \textit{марковская цепь} – это последовательность случайных событий, где вероятность следующего события зависит только от текущего. В этом смысле марковские цепи и  являются случайным процессом. В отличие от других случайных процессов, марковские процессы не сохраняют информацию о предыдущих событиях.

